{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data=[]\n",
    "        files=f.read()\n",
    "        files=files.lower()\n",
    "        files=nltk.word_tokenize(files)\n",
    "        data.extend(files)\n",
    "        \n",
    "    return data\n",
    "\n",
    "words=read_data('wikipedia2text-extracted.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000 \n",
    "count=[('UNK', -1)]\n",
    "count.extend(Counter(words).most_common(vocabulary_size-1))\n",
    "vocab2int={i:v for v, i in enumerate([word for word, _ in count])}\n",
    "int2vocab={i:v for i, v in zip(vocab2int.values(), vocab2int.keys())}\n",
    "data_ints=[vocab2int.get(val, -1) for val in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index=0\n",
    "def generate_batch(batch_size, window_size):\n",
    "    global data_index\n",
    "    batch=np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    span=2*window_size+1\n",
    "    num_samples=2*window_size\n",
    "    buffer=deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data_ints[data_index])\n",
    "        data_index=(data_index+1)%len(data_ints)\n",
    "        \n",
    "    for i in range(batch_size//num_samples): # m+1<= i <=N-m refer text\n",
    "        k=0\n",
    "        for j in list(range(window_size)) + list(range(window_size+1, span)):\n",
    "            batch[i*num_samples+k]=buffer[window_size]\n",
    "            labels[i*num_samples+k]=buffer[j]\n",
    "            k+=1\n",
    "            \n",
    "        buffer.append(data_ints[data_index])\n",
    "        data_index=(data_index+1)%len(data_ints)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=128\n",
    "batch_size=128\n",
    "window_size=4\n",
    "valid_size=16\n",
    "valid_window=50\n",
    "num_sampled=32\n",
    "epochs=100000\n",
    "\n",
    "valid_examples=np.array(random.sample(range(valid_window), valid_size)) #most commonly occuring words or opp case\n",
    "valid_examples=np.append(valid_samples, random.sample(range(1000, 1000+valid_window), valid_size), axis=0)#rare words or opp\n",
    "                                                                                                                #case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=0.5 / np.sqrt(embedding_size))\n",
    ")\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)#out shape = shape(parm2)+shape(par1)[:, 1:]\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "        labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)\n",
    ")\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings=embeddings/norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 50000)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #for epoch in range(epochs):\n",
    "    source, targets=generate_batch(batch_size, window_size)\n",
    "    sim=sess.run(similarity, feed_dict={train_dataset:source, train_labels:targets})\n",
    "    print(sim.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.2474678e-01 1.1076356e-01 3.9347810e-01 ... 1.1921988e-01\n",
      "  2.0631781e-01 1.1011354e-01]\n",
      " [6.1760038e-01 6.0697520e-01 1.2021061e-02 ... 8.5996085e-01\n",
      "  4.3269670e-01 1.4159871e-05]\n",
      " [8.7795031e-01 1.3306703e-01 3.8104516e-02 ... 6.6567194e-01\n",
      "  4.1667182e-02 3.1872335e-01]\n",
      " ...\n",
      " [3.4354240e-01 7.6964991e-03 6.8699151e-02 ... 3.6620507e-01\n",
      "  2.7619868e-05 4.8523460e-02]\n",
      " [7.5176734e-01 1.3101089e-04 2.8701469e-01 ... 4.0340790e-01\n",
      "  9.2573875e-01 8.6790049e-01]\n",
      " [2.1000512e-01 2.7518719e-01 2.3639007e-01 ... 9.2676628e-01\n",
      "  5.9659201e-01 4.1742063e-01]]\n"
     ]
    }
   ],
   "source": [
    "skip_losses=[]\n",
    "avg_loss=0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        source, targets=generate_batch(batch_size, window_size)\n",
    "        _, loss=sess.run([optm, loss], feed_dict=train_dataset:source, train_labels:targets)\n",
    "        avg_loss+=loss\n",
    "        \n",
    "        if (epoch+1)%2000==0:\n",
    "            avg_loss/=2000\n",
    "            print(\"Avg. loss at step %d = %f\", %(epoch+1, avg_loss))\n",
    "            skip_losses.append(avg_loss)\n",
    "            avg_loss=0\n",
    "            \n",
    "        if (epoch+1)%10000==0:\n",
    "            sim=similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word=int2vocab[valid_dataset[i]]\n",
    "                top_k=8\n",
    "                nearest=\n",
    "                for k in range(top_k):\n",
    "                    \n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
