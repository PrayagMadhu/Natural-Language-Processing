{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prayag/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, deque\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab\n",
    "from scipy.sparse import lil_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data=[]\n",
    "        files=f.read()\n",
    "        files=files.lower()\n",
    "        files=nltk.word_tokenize(files)\n",
    "        data.extend(files)\n",
    "        \n",
    "    return data\n",
    "\n",
    "words=read_data('/home/prayag/DL_projects/9781788478311_Code/NaturalLanguageProcessingwithTensorFlow_Code/ch3/wikipedia2text-extracted.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-03474b90e5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocab2int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mint2vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab2int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab2int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50001\n",
    "count=[('UNK', 50000)]\n",
    "count.extend(Counter(words).most_common(vocabulary_size-1))\n",
    "vocab2int={i:v for v, i in enumerate([word for word, _ in count])}\n",
    "int2vocab={i:v for i, v in zip(vocab2int.values(), vocab2int.keys())}\n",
    "data_ints=[vocab2int.get(val, 50000) for val in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index=0\n",
    "def generate_batch(batch_size, window_size):\n",
    "    global data_index\n",
    "    batch=np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    weights=np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    span=2*window_size+1\n",
    "    num_samples=2*window_size\n",
    "    buffer=deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data_ints[data_index])\n",
    "        data_index=(data_index+1)%len(data_ints)\n",
    "        \n",
    "    for i in range(batch_size//num_samples): # m+1<= i <=N-m refer text\n",
    "        k=0\n",
    "        for j in list(range(window_size)) + list(range(window_size+1, span)):\n",
    "            batch[i*num_samples+k]=buffer[window_size]\n",
    "            labels[i*num_samples+k]=buffer[j]\n",
    "            k+=1\n",
    "            \n",
    "        buffer.append(data_ints[data_index])\n",
    "        data_index=(data_index+1)%len(data_ints)\n",
    "        \n",
    "    return batch, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1451701 iterartion to calcualte cooc matrix\n",
      "100000 iteration\n",
      "200000 iteration\n",
      "300000 iteration\n",
      "400000 iteration\n",
      "500000 iteration\n",
      "600000 iteration\n",
      "700000 iteration\n",
      "800000 iteration\n",
      "900000 iteration\n",
      "1000000 iteration\n",
      "1100000 iteration\n",
      "1200000 iteration\n",
      "1300000 iteration\n",
      "1400000 iteration\n"
     ]
    }
   ],
   "source": [
    "dataset_size=len(data_ints)\n",
    "cooc_mat=lil_matrix((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "\n",
    "def generate_cooc(batch_size, window_size):\n",
    "    \n",
    "    print(\"running %d iterartion to calcualte cooc matrix\"%(dataset_size//batch_size))\n",
    "    \n",
    "    for i in range(dataset_size//batch_size):\n",
    "        if i>0 and i%100000==0:\n",
    "            print (\"%d iteration\"%(i))\n",
    "            \n",
    "        context, target, weights = generate_batch(8,4)\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        for c,t,w in zip(context,target,weights):\n",
    "            cooc_mat[c,t]+=1.0*w\n",
    "            \n",
    "generate_cooc(8,4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "embed_size=128\n",
    "window_size=4\n",
    "valid_size=16\n",
    "valid_window=50\n",
    "valid_sample=np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_sample=np.append(valid_sample, random.sample(range(1000, 1000+valid_window), valid_size), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_sample, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_embeddings=tf.Variable(tf.random_uniform([vocabulary_size, embed_size], -1.0, 1.0))\n",
    "in_bias_embeddings=tf.Variable(tf.random_uniform([vocabulary_size], 0.1, 0.01, dtype=tf.float32))\n",
    "out_embeddings=tf.Variable(tf.random_uniform([vocabulary_size, embed_size], -1.0, 1.0))\n",
    "out_bias_embeddings=tf.Variable(tf.random_uniform([vocabulary_size], 0.1, 0.01, dtype=tf.float32))\n",
    "\n",
    "embed_in=tf.nn.embedding_lookup(in_embeddings, train_dataset)\n",
    "embed_out=tf.nn.embedding_lookup(out_embeddings, train_labels)\n",
    "embed_bias_in=tf.nn.embedding_lookup(in_bias_embeddings, train_dataset)\n",
    "embed_bias_out=tf.nn.embedding_lookup(out_bias_embeddings, train_labels)\n",
    "\n",
    "weight_x=tf.placeholder(shape=[batch_size], dtype=tf.float32)\n",
    "x_ij=tf.placeholder(shape=[batch_size], dtype=tf.float32)\n",
    "\n",
    "loss=tf.reduce_mean(weight_x*(tf.reduce_sum(embed_in*embed_out, axis=0) +embed_bias_in+embed_bias_out-tf.log(1+x_ij))**2 )\n",
    "optm=tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(128)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-59e3f8e441fe>:2: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity\n",
    "embeddings=(in_embeddings+out_embeddings)/2\n",
    "norm=tf.sqrt(tf.reduce_mean(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "normalized_embeddings=embeddings/norm#a/|a| vocab_size X embed_size\n",
    "valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings, valid_dataset) #batch_size X embed_size\n",
    "similarity=tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) #batch_size X vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(50001)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=100001\n",
    "glove_loss=[]\n",
    "avg_loss=0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range (epochs):\n",
    "        batch_data, batch_labels, _ = generate_batch(batch_size, window_size)\n",
    "        batch_weight=[] #for calculating weighting term in paper, f(xij)\n",
    "        batch_xij=[] #for calculating cooccurance count of each pair of words in training batch\n",
    "        for c,t in zip(batch_data, batch_labels.reshape(-1)):\n",
    "            point_weight = (cooc_mat[c,t]/100)**0.75 if cooc_mat[c,t]<100 else 1.0\n",
    "            batch_weight.append(point_weight)\n",
    "            batch_xij.append(cooc_mat[c,t])\n",
    "            \n",
    "        batch_weight=np.clip(batch_weight, -100, 1)\n",
    "        batch_xij=np.asarray(batch_xij)\n",
    "        \n",
    "        _, l=sess.run([optm, loss], feed_dict:{train_dataset:batch_data, train_labels:batch_labels, \n",
    "                                              weight_x:batch_weight, x_ij:batch_xij})\n",
    "        \n",
    "        avg_loss+=l\n",
    "        if epoch%2000==0:\n",
    "            print(\"avg loss at %d is %d\"%(epoch, avg_loss))\n",
    "            glove_loss.append(avg_loss/2000)\n",
    "            avg_loss=0\n",
    "            \n",
    "        if epoch%10000==0:\n",
    "            #calculate close words for valid words\n",
    "            pass\n",
    "            \n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
